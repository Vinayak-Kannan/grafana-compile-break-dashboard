
TREE_GUARD_MANAGER:
+- RootGuardManager
| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:471 in init_ambient_guards
| +- GLOBAL_STATE: ___check_global_state()
| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()
| +- GuardManager: source=L['p0'], accessed_by=DictGetItemGuardAccessor(p0)
| | +- GuardManager: source=___from_numpy(L['p0']), accessed_by=PythonLambdaGuardAccessor
| | | +- TENSOR_MATCH: check_tensor(___from_numpy(L['p0']), Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.float64, device=None, requires_grad=False, size=[2], stride=[1])
| | | +- NO_HASATTR: hasattr(___from_numpy(L['p0']), '_dynamo_dynamic_indices') == False
| | | +- NO_TENSOR_ALIASING: check_no_aliasing(___from_numpy(L['p0']), ___from_numpy(L['mask']), ___from_numpy(L['___stack0']))
| +- GuardManager: source=L['mask'], accessed_by=DictGetItemGuardAccessor(mask)
| | +- GuardManager: source=___from_numpy(L['mask']), accessed_by=PythonLambdaGuardAccessor
| | | +- TENSOR_MATCH: check_tensor(___from_numpy(L['mask']), Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.bool, device=None, requires_grad=False, size=[2], stride=[1])
| | | +- NO_HASATTR: hasattr(___from_numpy(L['mask']), '_dynamo_dynamic_indices') == False
| | | +- NO_TENSOR_ALIASING
| +- GuardManager: source=L['___stack0'], accessed_by=DictGetItemGuardAccessor(___stack0)
| | +- GuardManager: source=___from_numpy(L['___stack0']), accessed_by=PythonLambdaGuardAccessor
| | | +- TENSOR_MATCH: check_tensor(___from_numpy(L['___stack0']), Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.float64, device=None, requires_grad=False, size=[0], stride=[0])
| | | +- NO_HASATTR: hasattr(___from_numpy(L['___stack0']), '_dynamo_dynamic_indices') == False
| | | +- NO_TENSOR_ALIASING
