# HELP python_gc_objects_collected_total Objects collected during gc
# TYPE python_gc_objects_collected_total counter
python_gc_objects_collected_total{generation="0"} 60181.0
python_gc_objects_collected_total{generation="1"} 84644.0
python_gc_objects_collected_total{generation="2"} 60611.0
# HELP python_gc_objects_uncollectable_total Uncollectable objects found during GC
# TYPE python_gc_objects_uncollectable_total counter
python_gc_objects_uncollectable_total{generation="0"} 0.0
python_gc_objects_uncollectable_total{generation="1"} 0.0
python_gc_objects_uncollectable_total{generation="2"} 0.0
# HELP python_gc_collections_total Number of times this generation was collected
# TYPE python_gc_collections_total counter
python_gc_collections_total{generation="0"} 2188.0
python_gc_collections_total{generation="1"} 237.0
python_gc_collections_total{generation="2"} 10.0
# HELP python_info Python platform information
# TYPE python_info gauge
python_info{implementation="CPython",major="3",minor="11",patchlevel="7",version="3.11.7"} 1.0
# HELP compile_breaks_total Torch.compile breaks per commit
# TYPE compile_breaks_total counter
compile_breaks_total{commit="1",model="prajjwal1/bert-tiny",reason="Failed to trace builtin operator\n  Explanation: Dynamo does not know how to trace builtin operator `print` with argument types [&#x27;&lt;unknown type&gt;&#x27;] (has_kwargs False)\n  Hint: Avoid calling builtin `print` with argument types [&#x27;&lt;unknown type&gt;&#x27;]. Consider using an equivalent alternative function/method to `print`.\n  Hint: If you are attempting to call a logging function (e.g. `print`), you can try adding it to `torch._dynamo.config.reorderable_logging_functions`.\n  Hint: Please report an issue to PyTorch.\n\n  Developer debug context: builtin print [&lt;class &#x27;torch._dynamo.variables.misc.StringFormatVariable&#x27;&gt;] False\n"} 1.0
compile_breaks_total{commit="2",model="prajjwal1/bert-tiny",reason="Failed to trace builtin operator\n  Explanation: Dynamo does not know how to trace builtin operator `print` with argument types [&#x27;&lt;unknown type&gt;&#x27;] (has_kwargs False)\n  Hint: Avoid calling builtin `print` with argument types [&#x27;&lt;unknown type&gt;&#x27;]. Consider using an equivalent alternative function/method to `print`.\n  Hint: If you are attempting to call a logging function (e.g. `print`), you can try adding it to `torch._dynamo.config.reorderable_logging_functions`.\n  Hint: Please report an issue to PyTorch.\n\n  Developer debug context: builtin print [&lt;class &#x27;torch._dynamo.variables.misc.StringFormatVariable&#x27;&gt;] False\n"} 1.0
compile_breaks_total{commit="3",model="prajjwal1/bert-tiny",reason="step_unsupported"} 1.0
compile_breaks_total{commit="4",model="prajjwal1/bert-tiny",reason="Dynamic shape operator\n  Explanation: Operator `aten.nonzero.default`&#x27;s output shape depends on input Tensor data.\n  Hint: Enable tracing of dynamic shape operators with `torch._dynamo.config.capture_dynamic_output_shape_ops = True`\n\n  Developer debug context: aten.nonzero.default\n"} 1.0
compile_breaks_total{commit="5",model="prajjwal1/bert-tiny",reason="Dynamic shape operator\n  Explanation: Operator `aten.nonzero.default`&#x27;s output shape depends on input Tensor data.\n  Hint: Enable tracing of dynamic shape operators with `torch._dynamo.config.capture_dynamic_output_shape_ops = True`\n\n  Developer debug context: aten.nonzero.default\n"} 1.0
compile_breaks_total{commit="6",model="prajjwal1/bert-tiny",reason="Dynamic shape operator\n  Explanation: Operator `aten.nonzero.default`&#x27;s output shape depends on input Tensor data.\n  Hint: Enable tracing of dynamic shape operators with `torch._dynamo.config.capture_dynamic_output_shape_ops = True`\n\n  Developer debug context: aten.nonzero.default\n"} 1.0
compile_breaks_total{commit="7",model="prajjwal1/bert-tiny",reason="Dynamic shape operator\n  Explanation: Operator `aten.nonzero.default`&#x27;s output shape depends on input Tensor data.\n  Hint: Enable tracing of dynamic shape operators with `torch._dynamo.config.capture_dynamic_output_shape_ops = True`\n\n  Developer debug context: aten.nonzero.default\n"} 1.0
compile_breaks_total{commit="8",model="prajjwal1/bert-tiny",reason="Dynamic shape operator\n  Explanation: Operator `aten.nonzero.default`&#x27;s output shape depends on input Tensor data.\n  Hint: Enable tracing of dynamic shape operators with `torch._dynamo.config.capture_dynamic_output_shape_ops = True`\n\n  Developer debug context: aten.nonzero.default\n"} 1.0
compile_breaks_total{commit="9",model="prajjwal1/bert-tiny",reason="Dynamic shape operator\n  Explanation: Operator `aten.nonzero.default`&#x27;s output shape depends on input Tensor data.\n  Hint: Enable tracing of dynamic shape operators with `torch._dynamo.config.capture_dynamic_output_shape_ops = True`\n\n  Developer debug context: aten.nonzero.default\n"} 1.0
compile_breaks_total{commit="10",model="prajjwal1/bert-tiny",reason="generic_jump NumpyNdarrayVariable()"} 1.0
compile_breaks_total{commit="11",model="prajjwal1/bert-tiny",reason="Data-dependent branching\n  Explanation: Detected data-dependent branching (e.g. `if my_tensor.sum() &gt; 0:`). Dynamo does not support tracing dynamic control flow.\n  Hint: This graph break is fundamental - it is unlikely that Dynamo will ever be able to trace through your code. Consider finding a workaround.\n  Hint: Use `torch.cond` to express dynamic control flow.\n\n  Developer debug context: attempted to jump with NumpyNdarrayVariable()\n"} 1.0
compile_breaks_total{commit="12",model="prajjwal1/bert-tiny",reason="Data-dependent branching\n  Explanation: Detected data-dependent branching (e.g. `if my_tensor.sum() &gt; 0:`). Dynamo does not support tracing dynamic control flow.\n  Hint: This graph break is fundamental - it is unlikely that Dynamo will ever be able to trace through your code. Consider finding a workaround.\n  Hint: Use `torch.cond` to express dynamic control flow.\n\n  Developer debug context: attempted to jump with NumpyNdarrayVariable()\n"} 1.0
compile_breaks_total{commit="13",model="prajjwal1/bert-tiny",reason="generic_jump NumpyNdarrayVariable()"} 1.0
compile_breaks_total{commit="14",model="prajjwal1/bert-tiny",reason="store_attr"} 1.0
compile_breaks_total{commit="15",model="prajjwal1/bert-tiny",reason="generic_jump NumpyNdarrayVariable()"} 1.0
compile_breaks_total{commit="16",model="prajjwal1/bert-tiny",reason="TODO: add support for ndarray.dtype"} 1.0
compile_breaks_total{commit="17",model="prajjwal1/bert-tiny",reason="generic_jump NumpyNdarrayVariable()"} 1.0
# HELP compile_breaks_created Torch.compile breaks per commit
# TYPE compile_breaks_created gauge
compile_breaks_created{commit="1",model="prajjwal1/bert-tiny",reason="Failed to trace builtin operator\n  Explanation: Dynamo does not know how to trace builtin operator `print` with argument types [&#x27;&lt;unknown type&gt;&#x27;] (has_kwargs False)\n  Hint: Avoid calling builtin `print` with argument types [&#x27;&lt;unknown type&gt;&#x27;]. Consider using an equivalent alternative function/method to `print`.\n  Hint: If you are attempting to call a logging function (e.g. `print`), you can try adding it to `torch._dynamo.config.reorderable_logging_functions`.\n  Hint: Please report an issue to PyTorch.\n\n  Developer debug context: builtin print [&lt;class &#x27;torch._dynamo.variables.misc.StringFormatVariable&#x27;&gt;] False\n"} 1.745721968944891e+09
compile_breaks_created{commit="2",model="prajjwal1/bert-tiny",reason="Failed to trace builtin operator\n  Explanation: Dynamo does not know how to trace builtin operator `print` with argument types [&#x27;&lt;unknown type&gt;&#x27;] (has_kwargs False)\n  Hint: Avoid calling builtin `print` with argument types [&#x27;&lt;unknown type&gt;&#x27;]. Consider using an equivalent alternative function/method to `print`.\n  Hint: If you are attempting to call a logging function (e.g. `print`), you can try adding it to `torch._dynamo.config.reorderable_logging_functions`.\n  Hint: Please report an issue to PyTorch.\n\n  Developer debug context: builtin print [&lt;class &#x27;torch._dynamo.variables.misc.StringFormatVariable&#x27;&gt;] False\n"} 1.7457219689452028e+09
compile_breaks_created{commit="3",model="prajjwal1/bert-tiny",reason="step_unsupported"} 1.745721968945261e+09
compile_breaks_created{commit="4",model="prajjwal1/bert-tiny",reason="Dynamic shape operator\n  Explanation: Operator `aten.nonzero.default`&#x27;s output shape depends on input Tensor data.\n  Hint: Enable tracing of dynamic shape operators with `torch._dynamo.config.capture_dynamic_output_shape_ops = True`\n\n  Developer debug context: aten.nonzero.default\n"} 1.745721968945303e+09
compile_breaks_created{commit="5",model="prajjwal1/bert-tiny",reason="Dynamic shape operator\n  Explanation: Operator `aten.nonzero.default`&#x27;s output shape depends on input Tensor data.\n  Hint: Enable tracing of dynamic shape operators with `torch._dynamo.config.capture_dynamic_output_shape_ops = True`\n\n  Developer debug context: aten.nonzero.default\n"} 1.7457219689453409e+09
compile_breaks_created{commit="6",model="prajjwal1/bert-tiny",reason="Dynamic shape operator\n  Explanation: Operator `aten.nonzero.default`&#x27;s output shape depends on input Tensor data.\n  Hint: Enable tracing of dynamic shape operators with `torch._dynamo.config.capture_dynamic_output_shape_ops = True`\n\n  Developer debug context: aten.nonzero.default\n"} 1.745721968945379e+09
compile_breaks_created{commit="7",model="prajjwal1/bert-tiny",reason="Dynamic shape operator\n  Explanation: Operator `aten.nonzero.default`&#x27;s output shape depends on input Tensor data.\n  Hint: Enable tracing of dynamic shape operators with `torch._dynamo.config.capture_dynamic_output_shape_ops = True`\n\n  Developer debug context: aten.nonzero.default\n"} 1.74572196894542e+09
compile_breaks_created{commit="8",model="prajjwal1/bert-tiny",reason="Dynamic shape operator\n  Explanation: Operator `aten.nonzero.default`&#x27;s output shape depends on input Tensor data.\n  Hint: Enable tracing of dynamic shape operators with `torch._dynamo.config.capture_dynamic_output_shape_ops = True`\n\n  Developer debug context: aten.nonzero.default\n"} 1.745721968945461e+09
compile_breaks_created{commit="9",model="prajjwal1/bert-tiny",reason="Dynamic shape operator\n  Explanation: Operator `aten.nonzero.default`&#x27;s output shape depends on input Tensor data.\n  Hint: Enable tracing of dynamic shape operators with `torch._dynamo.config.capture_dynamic_output_shape_ops = True`\n\n  Developer debug context: aten.nonzero.default\n"} 1.7457219689455009e+09
compile_breaks_created{commit="10",model="prajjwal1/bert-tiny",reason="generic_jump NumpyNdarrayVariable()"} 1.7457219689455402e+09
compile_breaks_created{commit="11",model="prajjwal1/bert-tiny",reason="Data-dependent branching\n  Explanation: Detected data-dependent branching (e.g. `if my_tensor.sum() &gt; 0:`). Dynamo does not support tracing dynamic control flow.\n  Hint: This graph break is fundamental - it is unlikely that Dynamo will ever be able to trace through your code. Consider finding a workaround.\n  Hint: Use `torch.cond` to express dynamic control flow.\n\n  Developer debug context: attempted to jump with NumpyNdarrayVariable()\n"} 1.745721968945581e+09
compile_breaks_created{commit="12",model="prajjwal1/bert-tiny",reason="Data-dependent branching\n  Explanation: Detected data-dependent branching (e.g. `if my_tensor.sum() &gt; 0:`). Dynamo does not support tracing dynamic control flow.\n  Hint: This graph break is fundamental - it is unlikely that Dynamo will ever be able to trace through your code. Consider finding a workaround.\n  Hint: Use `torch.cond` to express dynamic control flow.\n\n  Developer debug context: attempted to jump with NumpyNdarrayVariable()\n"} 1.745721968945645e+09
compile_breaks_created{commit="13",model="prajjwal1/bert-tiny",reason="generic_jump NumpyNdarrayVariable()"} 1.7457219689456892e+09
compile_breaks_created{commit="14",model="prajjwal1/bert-tiny",reason="store_attr"} 1.745721968945725e+09
compile_breaks_created{commit="15",model="prajjwal1/bert-tiny",reason="generic_jump NumpyNdarrayVariable()"} 1.7457219689458308e+09
compile_breaks_created{commit="16",model="prajjwal1/bert-tiny",reason="TODO: add support for ndarray.dtype"} 1.745721968945941e+09
compile_breaks_created{commit="17",model="prajjwal1/bert-tiny",reason="generic_jump NumpyNdarrayVariable()"} 1.745721968945985e+09
